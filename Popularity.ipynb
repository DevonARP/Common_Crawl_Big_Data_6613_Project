{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK629pcbv55Q",
        "outputId": "1d879ae7-18ca-45e5-b861-055de7468739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=f674313b7f2b85c99677768c0c2ac2e849736937a3a84e09e711928743ba64ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6hA_M2QwjGc",
        "outputId": "4a517861-f8c2-465d-bea5-d48104b93425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining pybloom from git+https://github.com/jaybaird/python-bloomfilter.git#egg=pybloom\n",
            "  Cloning https://github.com/jaybaird/python-bloomfilter.git to ./src/pybloom\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/jaybaird/python-bloomfilter.git /content/src/pybloom\n",
            "  Resolved https://github.com/jaybaird/python-bloomfilter.git to commit 2bbe01ad49965bf759e31781e6820408068862ac\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray>=0.3.4\n",
            "  Downloading bitarray-2.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.7/272.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitarray, pybloom\n",
            "  Running setup.py develop for pybloom\n",
            "Successfully installed bitarray-2.7.3 pybloom-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install -e git+https://github.com/jaybaird/python-bloomfilter.git#egg=pybloom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDoLY9oJwjR1",
        "outputId": "835c3855-7963-422f-ec2a-305e55a78f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting warcio\n",
            "  Downloading warcio-1.7.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from warcio) (1.16.0)\n",
            "Installing collected packages: warcio\n",
            "Successfully installed warcio-1.7.4\n"
          ]
        }
      ],
      "source": [
        "pip install warcio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvDpgrowwjcp",
        "outputId": "33801b06-2cd1-471e-a174-e08885a03ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.125-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.125\n",
            "  Downloading botocore-1.29.125-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.125->boto3) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.125->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.125->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.125 botocore-1.29.125 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rIXmTeHWwjma"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pyspark import sql as pysql\n",
        "from pyspark import SparkContext, SparkConf, SQLContext, streaming\n",
        "from pyspark.sql import functions as psqlf\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import urllib\n",
        "import traceback \n",
        "import pandas as pd\n",
        "import pybloom\n",
        "from warcio.archiveiterator import ArchiveIterator\n",
        "import gzip\n",
        "import boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "1xdQsjwBxArC",
        "outputId": "638427d0-39ab-4d77-9189-0b91a052bdc1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://580deb0b8d88:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>myApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f1719f42d40>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"myApp\") \\\n",
        "    .config(\"spark.driver.memory\", \"64g\") \\\n",
        "    .config(\"spark.executor.memory\", \"64g\") \\\n",
        "    .getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J6B7cC-wxfMh"
      },
      "outputs": [],
      "source": [
        "headers = ['WARC-Type',\n",
        "    'WARC-Target-URI',\n",
        "    'WARC-Date',\n",
        "    'WARC-Record-ID',\n",
        "    'WARC-Refers-To',\n",
        "    'WARC-Block-Digest',\n",
        "    'WARC-Identified-Content-Language',\n",
        "    'Content-Type',\n",
        "    'Content-Length'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CifntVnhxkqG"
      },
      "outputs": [],
      "source": [
        "ACCESS_KEY=\"\"\n",
        "SECRET_KEY=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "igRCKS_DxlNm"
      },
      "outputs": [],
      "source": [
        "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)\n",
        "obj = s3.get_object(Bucket= 'apbd5254', Key= 'keys_novdec_22.txt')\n",
        "file = obj['Body']\n",
        "file = file.read()\n",
        "file = file.decode(\"utf-8\")\n",
        "file = file.splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rb0rhuBxm1k",
        "outputId": "0c8b5f90-22ca-4063-c123-4094d90f9c61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': '2YK65FT7YH6ZKKCK',\n",
              "  'HostId': 'Y5tQWyt+nalkpFil6a/KtCVLf7lZ2/QK5qajkMMTSyLpeVz2X4nA90N6Vlhs6sEQZrMoVyuklrw=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'Y5tQWyt+nalkpFil6a/KtCVLf7lZ2/QK5qajkMMTSyLpeVz2X4nA90N6Vlhs6sEQZrMoVyuklrw=',\n",
              "   'x-amz-request-id': '2YK65FT7YH6ZKKCK',\n",
              "   'date': 'Tue, 02 May 2023 23:35:48 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"d27edd0223a208e76e863216bc15a0d2\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"d27edd0223a208e76e863216bc15a0d2\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'1' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2QpcVmS0wL4",
        "outputId": "b705743f-eaa7-40b8-e144-e0157f9bffae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'FB4ZCG18W4CEJG1J',\n",
              "  'HostId': 'cmfjXJVep0iPLiutwyNodqbuOrEWAzlll3U665qYiNHZp/HgM32C9YX1CQcgi7jGGVWnAUf4rfNUeAKKuvb8PA==',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'cmfjXJVep0iPLiutwyNodqbuOrEWAzlll3U665qYiNHZp/HgM32C9YX1CQcgi7jGGVWnAUf4rfNUeAKKuvb8PA==',\n",
              "   'x-amz-request-id': 'FB4ZCG18W4CEJG1J',\n",
              "   'date': 'Tue, 02 May 2023 23:43:55 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"8dc314f629e5045af8afd3fbca6fb1be\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"8dc314f629e5045af8afd3fbca6fb1be\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+20])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'2' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsm_A98r0wgY",
        "outputId": "067b37b6-6c82-43fc-aa7e-c6a5037a4df4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'W751MJ9A56Y89W7P',\n",
              "  'HostId': 'DgX8Ilq4T/keqGDvk4E9u8oXDEFPVZ6xdHnUXQurSq0UiMoqUmUOoCENRRZGvKIbSy4o+REG7L8=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'DgX8Ilq4T/keqGDvk4E9u8oXDEFPVZ6xdHnUXQurSq0UiMoqUmUOoCENRRZGvKIbSy4o+REG7L8=',\n",
              "   'x-amz-request-id': 'W751MJ9A56Y89W7P',\n",
              "   'date': 'Tue, 02 May 2023 23:48:53 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"7001455aa97fc59d43639a95ab5985aa\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"7001455aa97fc59d43639a95ab5985aa\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+40])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'3' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb7OdpqE0wwM",
        "outputId": "35b69959-4067-41e8-f3b1-364ed83a56ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': '9R61VZ6S09TXA1EX',\n",
              "  'HostId': 'Uw1YqyYFn2DrNDZo3fX8Eno8qJzz1w57gWmonLzcTz9C9YzH/xuiGEMVio4gmbRDLwR1foWUXU8=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'Uw1YqyYFn2DrNDZo3fX8Eno8qJzz1w57gWmonLzcTz9C9YzH/xuiGEMVio4gmbRDLwR1foWUXU8=',\n",
              "   'x-amz-request-id': '9R61VZ6S09TXA1EX',\n",
              "   'date': 'Tue, 02 May 2023 23:56:57 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"1bd4ea6430964cdec34ab6f2b377d1f5\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"1bd4ea6430964cdec34ab6f2b377d1f5\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+60])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'4' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcGAJR4l0zwi",
        "outputId": "959684cc-daac-4b73-9eb2-1e67a353c06f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': '7HY74QEQH67P90N9',\n",
              "  'HostId': 'S4ktaZN7ihm+vXMmgOOBxnwQfR1SdImsg2lVKs6bwyonClC5wC1rMECSFiorQM1ZQVzAPMDeRaU=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'S4ktaZN7ihm+vXMmgOOBxnwQfR1SdImsg2lVKs6bwyonClC5wC1rMECSFiorQM1ZQVzAPMDeRaU=',\n",
              "   'x-amz-request-id': '7HY74QEQH67P90N9',\n",
              "   'date': 'Wed, 03 May 2023 00:02:26 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"bb09ab752756e6b0699f4aaad9776f6e\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"bb09ab752756e6b0699f4aaad9776f6e\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+80])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'5' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6h-1TaL0w8E",
        "outputId": "22cc2a12-758d-4097-fe48-b196f0c2aa89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'Z76Y6S48P3MW93CZ',\n",
              "  'HostId': 'PL4o08x2Y0KMAhGU+6hk1z6rFy0TxFoij7uD175a30hBeIOqrb6TK2pymM5GHJtS2MIileFNp9Th6cneIapG7Q==',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'PL4o08x2Y0KMAhGU+6hk1z6rFy0TxFoij7uD175a30hBeIOqrb6TK2pymM5GHJtS2MIileFNp9Th6cneIapG7Q==',\n",
              "   'x-amz-request-id': 'Z76Y6S48P3MW93CZ',\n",
              "   'date': 'Wed, 03 May 2023 00:07:33 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"78efa0f45a4e19d941021e8e6f10f524\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"78efa0f45a4e19d941021e8e6f10f524\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+100])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'6' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9ICBQmb0xLR",
        "outputId": "d24f3111-5289-421c-b040-4b34ce84a7cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'TWV6W63MZP1TAJTN',\n",
              "  'HostId': '42Gdg3TM3v6OXSdFZQGF7RqLPZlqzFumMjQMVQfth18e/judoIARbF2DbvCRs4WAJE7bMRGIt02ltRrdVoxeSnTCcXGY3CxbSYXw2k7hnx8=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': '42Gdg3TM3v6OXSdFZQGF7RqLPZlqzFumMjQMVQfth18e/judoIARbF2DbvCRs4WAJE7bMRGIt02ltRrdVoxeSnTCcXGY3CxbSYXw2k7hnx8=',\n",
              "   'x-amz-request-id': 'TWV6W63MZP1TAJTN',\n",
              "   'date': 'Wed, 03 May 2023 00:12:47 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"7eb7a44dca870a44ce3a4c81e3402bb7\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"7eb7a44dca870a44ce3a4c81e3402bb7\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+120])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'7' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6o4HH8f0xXw",
        "outputId": "81735594-9787-455b-916b-508752c18f4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': '6Y8WK1SF0YSARVZ5',\n",
              "  'HostId': '3C6vI9apdl6MMUzyPMrUy8qgV9MpYzGNO6Kh2BXEgKfwM4taPhFhpXurI3t3WfYyWaXaqqOnjKM=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': '3C6vI9apdl6MMUzyPMrUy8qgV9MpYzGNO6Kh2BXEgKfwM4taPhFhpXurI3t3WfYyWaXaqqOnjKM=',\n",
              "   'x-amz-request-id': '6Y8WK1SF0YSARVZ5',\n",
              "   'date': 'Wed, 03 May 2023 00:18:12 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"76cc0ceb41687bc4650f4d4f21f92cdd\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"76cc0ceb41687bc4650f4d4f21f92cdd\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+140])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'8' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-tf8Ldp0xjb",
        "outputId": "f13a9439-f6cb-4b06-b13b-a107f15fd0f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'T7J5P8X4BRJQ8E1Y',\n",
              "  'HostId': 'm2h5IrQUSVN3wCWLdBxEp3krPIhhDIjSg+daHdhWI2CDdj+f1R0oPGhoiMBwdWBNJVq8w3ciKLE=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'm2h5IrQUSVN3wCWLdBxEp3krPIhhDIjSg+daHdhWI2CDdj+f1R0oPGhoiMBwdWBNJVq8w3ciKLE=',\n",
              "   'x-amz-request-id': 'T7J5P8X4BRJQ8E1Y',\n",
              "   'date': 'Wed, 03 May 2023 00:23:43 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"74d25b4affede7570abff7f46a67909f\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"74d25b4affede7570abff7f46a67909f\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+160])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'9' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQhFGAHu0xv1",
        "outputId": "a381d5fe-a454-4dab-d3e5-64c55aba63cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': '3FPN1W0C5HFGW682',\n",
              "  'HostId': 'fBrEuefWBC+BsRul+HNIaz87xe+i+Vd/YGfay4Qa4Lf636L0/8Jc6Du6EyUms53q8uHXKOilT8A=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'fBrEuefWBC+BsRul+HNIaz87xe+i+Vd/YGfay4Qa4Lf636L0/8Jc6Du6EyUms53q8uHXKOilT8A=',\n",
              "   'x-amz-request-id': '3FPN1W0C5HFGW682',\n",
              "   'date': 'Wed, 03 May 2023 00:29:03 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"afac726b7a7a0598056d0fa4b3f7fb24\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"afac726b7a7a0598056d0fa4b3f7fb24\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+180])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "def get_refed_websites(content):\n",
        "    urls = re.findall(url_pattern, content)\n",
        "    return urls\n",
        "spark_content = spark_df.select('Content')\n",
        "spark_url_list = spark_content.rdd.flatMap(lambda a: get_refed_websites(a['Content']))\n",
        "spark_hostnames = spark_url_list.map(lambda a: urllib.parse.urlparse(a).hostname)\n",
        "spark_hostname_counts = spark_hostnames.countByValue().items()\n",
        "spark_hostname_counts = sorted(spark_hostname_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_hostname_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='Popularity/'+'10' +'.txt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
