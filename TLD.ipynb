{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-5jzY1I9TU9",
        "outputId": "77d3dfe2-8bb5-4950-ff6e-0205d70cc583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=47968b4dc81180640d65085ce105e7e4217d2cbe8af0233b75c7e897c02ec35d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmeqnKyP9sbf",
        "outputId": "e30f1305-fd93-49d4-d1b4-25260b80d47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining pybloom from git+https://github.com/jaybaird/python-bloomfilter.git#egg=pybloom\n",
            "  Cloning https://github.com/jaybaird/python-bloomfilter.git to ./src/pybloom\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/jaybaird/python-bloomfilter.git /content/src/pybloom\n",
            "  Resolved https://github.com/jaybaird/python-bloomfilter.git to commit 2bbe01ad49965bf759e31781e6820408068862ac\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: bitarray>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from pybloom) (2.7.3)\n",
            "Installing collected packages: pybloom\n",
            "  Running setup.py develop for pybloom\n",
            "Successfully installed pybloom-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install -e git+https://github.com/jaybaird/python-bloomfilter.git#egg=pybloom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKng55vLJPAk",
        "outputId": "f29e650c-5f5f-4a8a-ae3d-598d6e3fb971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting warcio\n",
            "  Downloading warcio-1.7.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from warcio) (1.16.0)\n",
            "Installing collected packages: warcio\n",
            "Successfully installed warcio-1.7.4\n"
          ]
        }
      ],
      "source": [
        "pip install warcio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOOBIeEOJUBu",
        "outputId": "84aa2986-53d3-43dd-f8f7-911f22ff8c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.125-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.125\n",
            "  Downloading botocore-1.29.125-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.125->boto3) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.125->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.125->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.125 botocore-1.29.125 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW7neQ_rNn7C"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y3BkiqWA9bGP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark import sql as pysql\n",
        "from pyspark import SparkContext, SparkConf, SQLContext, streaming\n",
        "from pyspark.sql import functions as psqlf\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import pandas as pd\n",
        "import pybloom\n",
        "from warcio.archiveiterator import ArchiveIterator\n",
        "import gzip\n",
        "import boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Nj1nuHBsLNVf",
        "outputId": "1b82c569-f91e-4fe9-9089-61f833de4e94"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ab557ee115b6:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>myApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2184929a20>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"myApp\") \\\n",
        "    .config(\"spark.driver.memory\", \"64g\") \\\n",
        "    .config(\"spark.executor.memory\", \"64g\") \\\n",
        "    .getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4AO0nVyU9X-9"
      },
      "outputs": [],
      "source": [
        "headers = ['WARC-Type',\n",
        "    'WARC-Target-URI',\n",
        "    'WARC-Date',\n",
        "    'WARC-Record-ID',\n",
        "    'WARC-Refers-To',\n",
        "    'WARC-Block-Digest',\n",
        "    'WARC-Identified-Content-Language',\n",
        "    'Content-Type',\n",
        "    'Content-Length'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9JDlrItWNH0I"
      },
      "outputs": [],
      "source": [
        "ACCESS_KEY=\"\"\n",
        "SECRET_KEY=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IfPfcVmGNI4-"
      },
      "outputs": [],
      "source": [
        "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)\n",
        "obj = s3.get_object(Bucket= 'apbd5254', Key= 'keys_novdec_22.txt')\n",
        "file = obj['Body']\n",
        "file = file.read()\n",
        "file = file.decode(\"utf-8\")\n",
        "file = file.splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1wJOwQAJfbO",
        "outputId": "7f9eb8e6-5e9b-4270-acf0-473f10dd2e8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'A57KF4FGGKAWJ7AY',\n",
              "  'HostId': 'sZszmqHH78PcBwVr8q1tTCUEE56+u6aC6O8WqeYDUOaWtGdu8i5axR+Xnk2mWgRF04LbOjhYj9UPvkEC6nT1ow==',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'sZszmqHH78PcBwVr8q1tTCUEE56+u6aC6O8WqeYDUOaWtGdu8i5axR+Xnk2mWgRF04LbOjhYj9UPvkEC6nT1ow==',\n",
              "   'x-amz-request-id': 'A57KF4FGGKAWJ7AY',\n",
              "   'date': 'Tue, 02 May 2023 22:22:25 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"fc5f820b3d4f6bab8cb1a5e1471c5874\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"fc5f820b3d4f6bab8cb1a5e1471c5874\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'1' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtXFrsrWd2ss",
        "outputId": "5fa58c75-0362-491a-cc97-deb2f6807638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'N5H3VP2PXNJGH519',\n",
              "  'HostId': 'wk97nrs0fMcFLUsCz3AQz2l8gCCu1ErdjTZK05OH/Ct6kzDZUffkyiD4C9MYqWpKQ0B384W7MAo=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'wk97nrs0fMcFLUsCz3AQz2l8gCCu1ErdjTZK05OH/Ct6kzDZUffkyiD4C9MYqWpKQ0B384W7MAo=',\n",
              "   'x-amz-request-id': 'N5H3VP2PXNJGH519',\n",
              "   'date': 'Tue, 02 May 2023 22:28:09 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"4dc663330d68d1ccde956392c0ddd78b\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"4dc663330d68d1ccde956392c0ddd78b\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+20])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'2' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxQrrrRkd26r",
        "outputId": "e6cc0523-7308-4605-ff17-685501a5cf98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'Y7P5S3AQWSHKC3EH',\n",
              "  'HostId': 'NMr7wWTcDDTW2CkzkDw74VsrVwOW0sx2vlU6DjAVMMSDtBdtgs/qfyI9kOr9cPO0t8Qvk23izFg=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'NMr7wWTcDDTW2CkzkDw74VsrVwOW0sx2vlU6DjAVMMSDtBdtgs/qfyI9kOr9cPO0t8Qvk23izFg=',\n",
              "   'x-amz-request-id': 'Y7P5S3AQWSHKC3EH',\n",
              "   'date': 'Tue, 02 May 2023 22:33:37 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"a667d5df5219ae5a7e1f467a9dcd9f87\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"a667d5df5219ae5a7e1f467a9dcd9f87\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+40])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'3' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j2ZFRvcd3Fw",
        "outputId": "b7885fc0-4dcc-45eb-ffab-1fbded7627f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'S19QF2DYKHQX7RXX',\n",
              "  'HostId': 'BCWaWPjxjheZAx309rgGi8gcBiSfUa+UxTPBJaHBD/lgUlVjAkioVcg0fQDbHzRLTY2qOJ9WPX8=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'BCWaWPjxjheZAx309rgGi8gcBiSfUa+UxTPBJaHBD/lgUlVjAkioVcg0fQDbHzRLTY2qOJ9WPX8=',\n",
              "   'x-amz-request-id': 'S19QF2DYKHQX7RXX',\n",
              "   'date': 'Tue, 02 May 2023 22:40:04 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"a54ad50ed44ee27d496eebde357c728e\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"a54ad50ed44ee27d496eebde357c728e\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+60])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'4' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SZzDNUAd3TR",
        "outputId": "5fddc57c-3085-45f1-b60f-0106d1145fbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'QXQ8DCED5CCH8B51',\n",
              "  'HostId': 'zwoToV/PQak13M/RvDEfknzShMSXQLjpvzDtgOyY/hj28ti3WFAplQmOYMBN7353/SIhrvce1Xk=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'zwoToV/PQak13M/RvDEfknzShMSXQLjpvzDtgOyY/hj28ti3WFAplQmOYMBN7353/SIhrvce1Xk=',\n",
              "   'x-amz-request-id': 'QXQ8DCED5CCH8B51',\n",
              "   'date': 'Tue, 02 May 2023 22:45:06 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"8e2cd215ffd3c52475dc0b5f769027d7\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"8e2cd215ffd3c52475dc0b5f769027d7\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+80])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'5' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyvYhIyNd3gS",
        "outputId": "d2f8f172-726a-43e2-89f6-929f2705eeb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': '5DXJAC1MG6A099BC',\n",
              "  'HostId': 'ZSVwLNNDO0CK0abpF2G65QSBxspyzObwd7/oPU9eHaCWX670wBcxp1i2uQ25MgtQREKcQqIMook=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'ZSVwLNNDO0CK0abpF2G65QSBxspyzObwd7/oPU9eHaCWX670wBcxp1i2uQ25MgtQREKcQqIMook=',\n",
              "   'x-amz-request-id': '5DXJAC1MG6A099BC',\n",
              "   'date': 'Tue, 02 May 2023 22:50:05 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"5d2891ce47d628ec3661bc30aa973ea8\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"5d2891ce47d628ec3661bc30aa973ea8\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+100])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'6' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpXExwXod3rC",
        "outputId": "d4885766-1d47-40bc-cae6-8828e9f5e882"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'R458HCY3Q6B3E7HW',\n",
              "  'HostId': 'vqOtagHxupIkq+OQbwbNZ0OZlLcul58u4W2e8urJckaIrBmrS5QalAkNctFyVr+ZDawJoWbp1Vk=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'vqOtagHxupIkq+OQbwbNZ0OZlLcul58u4W2e8urJckaIrBmrS5QalAkNctFyVr+ZDawJoWbp1Vk=',\n",
              "   'x-amz-request-id': 'R458HCY3Q6B3E7HW',\n",
              "   'date': 'Tue, 02 May 2023 22:56:24 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"795a102f7229138c804d205ab9fdc489\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"795a102f7229138c804d205ab9fdc489\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+120])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'7' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9hNpfqxd32j",
        "outputId": "a1913e90-fd2b-44bb-fe79-689fc99ab5db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'WEEYBKN1Y6F7RVSM',\n",
              "  'HostId': '5o1cmYhS2qbhq3tksREwn6mzGx2LDd7pkzteSC2euLmntd/SRAsbCY00YRjT2mBOVyZVZMbMMjk=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': '5o1cmYhS2qbhq3tksREwn6mzGx2LDd7pkzteSC2euLmntd/SRAsbCY00YRjT2mBOVyZVZMbMMjk=',\n",
              "   'x-amz-request-id': 'WEEYBKN1Y6F7RVSM',\n",
              "   'date': 'Tue, 02 May 2023 23:01:20 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"7a8cec81729a46e2c234f67f2ac4daec\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"7a8cec81729a46e2c234f67f2ac4daec\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+140])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'8' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E9A9Noqd4B3",
        "outputId": "92dc1d97-d585-4730-d58e-d46a3cb2501d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': '14NVN5PMCW1B7XEJ',\n",
              "  'HostId': 'LxoHLRc9WcXzqFBKsU/hbvuRWxF0c63TLcsq3nT7yrcxE0kJixv4WzgMWG1wV930ByqmX8/Ckto=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'LxoHLRc9WcXzqFBKsU/hbvuRWxF0c63TLcsq3nT7yrcxE0kJixv4WzgMWG1wV930ByqmX8/Ckto=',\n",
              "   'x-amz-request-id': '14NVN5PMCW1B7XEJ',\n",
              "   'date': 'Tue, 02 May 2023 23:06:16 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"23d32661fb546fb8696275e4b5142ded\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"23d32661fb546fb8696275e4b5142ded\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+160])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'9' +'.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8_JoKaId4Mw",
        "outputId": "7fc81616-55ee-45d3-987f-8cfbc0e475c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ResponseMetadata': {'RequestId': 'DEBGRVQA66XAW9S9',\n",
              "  'HostId': 'sqRWQTbDA797HV4qjPneNM58Abafzi8IhrtXRYKs0b8YJxOjyaG4R3ODlhkQQ/jlZrQDbFPnVHlW+IB6OGTpcstefGib8m+T/dYk9MAX9G4=',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amz-id-2': 'sqRWQTbDA797HV4qjPneNM58Abafzi8IhrtXRYKs0b8YJxOjyaG4R3ODlhkQQ/jlZrQDbFPnVHlW+IB6OGTpcstefGib8m+T/dYk9MAX9G4=',\n",
              "   'x-amz-request-id': 'DEBGRVQA66XAW9S9',\n",
              "   'date': 'Tue, 02 May 2023 23:11:14 GMT',\n",
              "   'x-amz-server-side-encryption': 'AES256',\n",
              "   'etag': '\"34f6069aca62a5f9b31919f3a7da3cb2\"',\n",
              "   'server': 'AmazonS3',\n",
              "   'content-length': '0'},\n",
              "  'RetryAttempts': 0},\n",
              " 'ETag': '\"34f6069aca62a5f9b31919f3a7da3cb2\"',\n",
              " 'ServerSideEncryption': 'AES256'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "records = []\n",
        "for i in range(20):\n",
        "  obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i+180])\n",
        "  with gzip.open(obj['Body'], 'r') as stream:\n",
        "      for record in ArchiveIterator(stream):\n",
        "          _record = dict(record.rec_headers.headers)\n",
        "          _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "          records.append(_record)\n",
        "  records_df = pd.DataFrame(records)\n",
        "\n",
        "#drop na on uri columns\n",
        "records_df = records_df.dropna(axis=0, subset=['WARC-Target-URI'])\n",
        "records_df['WARC-Identified-Content-Language'] = records_df['WARC-Identified-Content-Language'].fillna('not-available')\n",
        "\n",
        "spark_df = spark.createDataFrame(records_df)\n",
        "import urllib\n",
        "import traceback \n",
        "\n",
        "def get_tld(uri):\n",
        "    try:\n",
        "        uri_parsed = urllib.parse.urlparse(uri)\n",
        "        hostname = uri_parsed.hostname\n",
        "        tld = hostname.split('.')[-1]\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(traceback.format_exc())\n",
        "        return '.'\n",
        "    return tld\n",
        "spark_uris = spark_df.select('WARC-Target-URI')\n",
        "spark_tlds = spark_uris.rdd.map(lambda a : get_tld(a['WARC-Target-URI']))\n",
        "spark_tld_counts = spark_tlds.countByValue().items()\n",
        "spark_tld_counts = sorted(spark_tld_counts, key=(lambda a: a[1]), reverse=True)\n",
        "s3.put_object(\n",
        "Body=str(spark_tld_counts), \n",
        "Bucket='apbd5254', \n",
        "Key='TopLevelDomain/'+'10' +'.txt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
