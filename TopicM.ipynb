{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POCpKQ7Zpz5u",
        "outputId": "5ae91874-f347-4da0-f704-62ec2ddb3aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting warcio\n",
            "  Downloading warcio-1.7.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from warcio) (1.16.0)\n",
            "Installing collected packages: warcio\n",
            "Successfully installed warcio-1.7.4\n"
          ]
        }
      ],
      "source": [
        "pip install warcio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF6bfzFUqKzg",
        "outputId": "032a5eba-4320-44e2-ae42-329dab653319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL-KBEpVqLW-",
        "outputId": "50495cb0-99bd-4d57-bad4-260b60adf890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.125-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.125\n",
            "  Downloading botocore-1.29.125-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.125->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.125->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.125->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.125 botocore-1.29.125 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWfFUlr6qNjB",
        "outputId": "0b306900-5b70-49fc-c2e7-afc0c3ba82db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=f7e0ec56c0e10d75ed1e9c989fe6f5735df980e3e43f355493d48498a7791231\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "T4wkSva_qPhD",
        "outputId": "9fc13d8b-d381-4eac-c2fe-253fd1b5b789"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://15ba84cc0e43:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>myApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7204c6b370>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "import warcio\n",
        "import re\n",
        "import nltk\n",
        "import gzip\n",
        "import boto3\n",
        "nltk.download('words')\n",
        "from warcio.archiveiterator import ArchiveIterator\n",
        "from nltk.corpus import words\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"myApp\") \\\n",
        "    .config(\"spark.driver.memory\", \"64g\") \\\n",
        "    .config(\"spark.executor.memory\", \"64g\") \\\n",
        "    .getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yANaMnbcqvRb"
      },
      "outputs": [],
      "source": [
        "schema = StructType([\n",
        "    StructField(\"WARC-Type\", StringType(), True),\n",
        "    StructField(\"WARC-Target-URI\", StringType(), True),\n",
        "    StructField(\"WARC-Date\", StringType(), True),\n",
        "    StructField(\"WARC-Record-ID\", StringType(), True),\n",
        "    StructField(\"WARC-Refers-To\", StringType(), True),\n",
        "    StructField(\"WARC-Block-Digest\", StringType(), True),\n",
        "    StructField(\"WARC-Identified-Content-Language\", StringType(), True),\n",
        "    StructField(\"Content-Type\", StringType(), True),\n",
        "    StructField(\"Content-Length\", IntegerType(), True),\n",
        "    StructField(\"Content\", StringType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVXTw_njq1Fr"
      },
      "outputs": [],
      "source": [
        "ACCESS_KEY=\"\"\n",
        "SECRET_KEY=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LQSf8eMq1hU",
        "outputId": "f0bdb91d-cf61-4d95-ab35-fa0dd80e2d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apbd5254\n",
            "aws-emr-resources-411926377614-us-east-1\n",
            "aws-logs-411926377614-us-east-1\n",
            "elasticbeanstalk-us-east-1-411926377614\n",
            "elasticbeanstalk-us-west-2-411926377614\n"
          ]
        }
      ],
      "source": [
        "s3 = boto3.resource('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)\n",
        "for bucket in s3.buckets.all():\n",
        "    print(bucket.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsGw7B5Vq2dG"
      },
      "outputs": [],
      "source": [
        "keys=[]\n",
        "for obj in s3.Bucket('apbd5254').objects.all():\n",
        "    keys.append(obj.key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksk2cxWqq3ob"
      },
      "outputs": [],
      "source": [
        "s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)\n",
        "obj = s3.get_object(Bucket= 'apbd5254', Key= 'keys_novdec_22.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy_kniJEq4ZY"
      },
      "outputs": [],
      "source": [
        "file = obj['Body']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpuZ-75yq5NY"
      },
      "outputs": [],
      "source": [
        "file = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oBIHEzCq6GC"
      },
      "outputs": [],
      "source": [
        "file = file.decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "847a_Oaqq634"
      },
      "outputs": [],
      "source": [
        "file = file.splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4EQgYp1sQU9"
      },
      "outputs": [],
      "source": [
        "for i in range(200): #1TB\n",
        "    records = []\n",
        "    obj = s3.get_object(Bucket= 'commoncrawl', Key= file[i])\n",
        "    with gzip.open(obj['Body'], 'r') as stream:\n",
        "        for record in ArchiveIterator(stream):\n",
        "            _record = dict(record.rec_headers.headers)\n",
        "            _record['Content'] = record.raw_stream.read().decode('utf-8')\n",
        "            records.append(_record)\n",
        "    spark_df = spark.createDataFrame(records)\n",
        "    tokenizer = RegexTokenizer(inputCol=\"Content\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "    tokenized_df = tokenizer.transform(spark_df)\n",
        "    stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "    filtered_df = stopwords_remover.transform(tokenized_df)\n",
        "    english_words = set(words.words())\n",
        "    def filter_english_words(words):\n",
        "        return [word for word in words if word.lower() in english_words]\n",
        "    filter_english_words_udf = udf(filter_english_words, ArrayType(StringType()))\n",
        "    filtered_english_words_df = filtered_df.withColumn(\"english_words\", filter_english_words_udf(\"filtered\"))\n",
        "    english_words = set(words.words())\n",
        "    def filter_english_words(words):\n",
        "        return [word for word in words if word.lower() in english_words]\n",
        "    filter_english_words_udf = udf(filter_english_words, ArrayType(StringType()))\n",
        "    filtered_english_words_df = filtered_df.withColumn(\"english_words\", filter_english_words_udf(\"filtered\"))\n",
        "    vectorizer = CountVectorizer(inputCol=\"english_words\", outputCol=\"rawFeatures\", vocabSize=5000, minDF=5.0)\n",
        "    vectorized_df = vectorizer.fit(filtered_english_words_df).transform(filtered_english_words_df)\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    idf_model = idf.fit(vectorized_df)\n",
        "    tfidf_vectors = idf_model.transform(vectorized_df)\n",
        "    num_topics = 5\n",
        "    lda = LDA(k=num_topics, maxIter=10)\n",
        "    lda_model = lda.fit(tfidf_vectors)\n",
        "    vectorizer_model = vectorizer.fit(filtered_english_words_df)\n",
        "    vocab = vectorizer_model.vocabulary\n",
        "    topics = lda_model.describeTopics(maxTermsPerTopic=20)\n",
        "    topic_terms_udf = udf(lambda indices: [vocab[i] for i in indices], ArrayType(StringType()))\n",
        "    topics_with_terms = topics.withColumn(\"topic_terms\", topic_terms_udf(\"termIndices\"))\n",
        "    n = 3\n",
        "    top_topics = topics_with_terms.select(\"topic\", \"topic_terms\").limit(n).collect()\n",
        "    top_topics_list = [(row.topic, row.topic_terms) for row in top_topics]\n",
        "    string = \"The top \" + str(n) + \" topics are: \" + str(top_topics_list)\n",
        "    s3.put_object(\n",
        "    Body=string, \n",
        "    Bucket='apbd5254', \n",
        "    Key='TopicModel/'+str(i) +'.txt'\n",
        "  )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
